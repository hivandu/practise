{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 基于SVM的文本分类实战\n",
    "本教程告诉你如何使用SVM来实现一个基于词袋和支持向量机的简单文本分类。\n",
    "## 载入数据\n",
    "我们准备了中文的新闻数据作为样例数据集，其中训练数据条数50000，测试数据条数10000，所有数据分为体育、财经、房产、家居、教育、科技、时尚、时政、游戏和娱乐10类。从训练文本中，可以载入代码，查看数据格式和样例："
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import codecs\n",
    "import os\n",
    "import jieba"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "train_file = '../resource/cnews/cnews.train.txt' # training data file name  \n",
    "test_file = '../resource/cnews/cnews.test.txt'\n",
    "vocab = '../resource/cnews/cnews.vocab.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "with codecs.open(train_file, 'r', 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# print sample content\n",
    "label, content = lines[0].strip('\\r\\n').split('\\t')\n",
    "print(content)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有摆脱雨水的困扰。7月31日下午6点，国奥队的日常训练再度受到大雨的干扰，无奈之下队员们只慢跑了25分钟就草草收场。31日上午10点，国奥队在奥体中心外场训练的时候，天就是阴沉沉的，气象预报显示当天下午沈阳就有大雨，但幸好队伍上午的训练并没有受到任何干扰。下午6点，当球队抵达训练场时，大雨已经下了几个小时，而且丝毫没有停下来的意思。抱着试一试的态度，球队开始了当天下午的例行训练，25分钟过去了，天气没有任何转好的迹象，为了保护球员们，国奥队决定中止当天的训练，全队立即返回酒店。在雨中训练对足球队来说并不是什么稀罕事，但在奥运会即将开始之前，全队变得“娇贵”了。在沈阳最后一周的训练，国奥队首先要保证现有的球员不再出现意外的伤病情况以免影响正式比赛，因此这一阶段控制训练受伤、控制感冒等疾病的出现被队伍放在了相当重要的位置。而抵达沈阳之后，中后卫冯萧霆就一直没有训练，冯萧霆是7月27日在长春患上了感冒，因此也没有参加29日跟塞尔维亚的热身赛。队伍介绍说，冯萧霆并没有出现发烧症状，但为了安全起见，这两天还是让他静养休息，等感冒彻底好了之后再恢复训练。由于有了冯萧霆这个例子，因此国奥队对雨中训练就显得特别谨慎，主要是担心球员们受凉而引发感冒，造成非战斗减员。而女足队员马晓旭在热身赛中受伤导致无缘奥运的前科，也让在沈阳的国奥队现在格外警惕，“训练中不断嘱咐队员们要注意动作，我们可不能再出这样的事情了。”一位工作人员表示。从长春到沈阳，雨水一路伴随着国奥队，“也邪了，我们走到哪儿雨就下到哪儿，在长春几次训练都被大雨给搅和了，没想到来沈阳又碰到这种事情。”一位国奥球员也对雨水的“青睐”有些不解。\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "以训练数据的第一条为例，对载入的新闻数据进行分词，在这里我使用的LTP的切词功能，你也可以使用jieba，分词结果用\"/\"符号隔开展示。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# print word segment results\n",
    "segment = jieba.cut(content)\n",
    "print('/'.join(segment))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "马晓旭/意外/受伤/让/国奥/警惕/ /无奈/大雨/格外/青睐/殷家/军/记者/傅亚雨/沈阳/报道/ /来到/沈阳/，/国奥队/依然/没有/摆脱/雨水/的/困扰/。/7/月/31/日/下午/6/点/，/国奥队/的/日常/训练/再度/受到/大雨/的/干扰/，/无奈/之下/队员/们/只/慢跑/了/25/分钟/就/草草收场/。/31/日/上午/10/点/，/国奥队/在/奥体中心/外场/训练/的/时候/，/天/就是/阴沉沉/的/，/气象预报/显示/当天/下午/沈阳/就/有/大雨/，/但/幸好/队伍/上午/的/训练/并/没有/受到/任何/干扰/。/下午/6/点/，/当/球队/抵达/训练场/时/，/大雨/已经/下/了/几个/小时/，/而且/丝毫/没有/停下来/的/意思/。/抱/着/试一试/的/态度/，/球队/开始/了/当天/下午/的/例行/训练/，/25/分钟/过去/了/，/天气/没有/任何/转好/的/迹象/，/为了/保护/球员/们/，/国奥队/决定/中止/当天/的/训练/，/全队/立即/返回/酒店/。/在/雨/中/训练/对/足球队/来说/并/不是/什么/稀罕/事/，/但/在/奥运会/即将/开始/之前/，/全队/变得/“/娇贵/”/了/。/在/沈阳/最后/一周/的/训练/，/国奥队/首先/要/保证/现有/的/球员/不再/出现意外/的/伤病/情况/以免/影响/正式/比赛/，/因此/这一/阶段/控制/训练/受伤/、/控制/感冒/等/疾病/的/出现/被/队伍/放在/了/相当/重要/的/位置/。/而/抵达/沈阳/之后/，/中/后卫/冯萧霆/就/一直/没有/训练/，/冯萧霆/是/7/月/27/日/在/长春/患上/了/感冒/，/因此/也/没有/参加/29/日/跟/塞尔维亚/的/热身赛/。/队伍/介绍/说/，/冯萧霆/并/没有/出现/发烧/症状/，/但/为了/安全/起/见/，/这/两天/还是/让/他/静养/休息/，/等/感冒/彻底/好/了/之后/再/恢复/训练/。/由于/有/了/冯萧霆/这个/例子/，/因此/国奥队/对雨中/训练/就/显得/特别/谨慎/，/主要/是/担心/球员/们/受凉/而/引发/感冒/，/造成/非战斗/减员/。/而/女足/队员/马晓旭/在/热身赛/中/受伤/导致/无缘/奥运/的/前科/，/也/让/在/沈阳/的/国奥队/现在/格外/警惕/，/“/训练/中/不断/嘱咐/队员/们/要/注意/动作/，/我们/可/不能/再出/这样/的/事情/了/。/”/一位/工作人员/表示/。/从/长春/到/沈阳/，/雨水/一路/伴随/着/国奥队/，/“/也/邪/了/，/我们/走/到/哪儿/雨/就/下/到/哪儿/，/在/长春/几次/训练/都/被/大雨/给/搅和/了/，/没想到/来/沈阳/又/碰到/这种/事情/。/”/一位/国奥/球员/也/对/雨水/的/“/青睐/”/有些/不解/。\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "把上述逻辑稍微整理一下，实现一个类来载入训练和测试数据并进行分词。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# cut data\n",
    "def process_line(idx, line):\n",
    "    data = tuple(line.strip('\\r\\n').split('\\t'))\n",
    "    if not len(data)  == 2:\n",
    "        return None\n",
    "    content_segged = list(jieba.cut(data[1]))\n",
    "    if idx % 1000 == 0:\n",
    "        print('line number: {}'.format(idx))\n",
    "    return (data[0], content_segged)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# data loading method\n",
    "def load_data(file):\n",
    "    with codecs.open(file, 'r', 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    data_records = [process_line(idx, line) for idx, line in enumerate(lines)]\n",
    "    data_records = [data for data in data_records if data is not None]\n",
    "    return data_records"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# load and process training data\n",
    "train_data = load_data(train_file)\n",
    "print('first training data: label {} segment {}'.format(train_data[0][0], '/'.join(train_data[0][1])))\n",
    "\n",
    "# load and process testing data\n",
    "test_data = load_data(test_file)\n",
    "print('first testing data: label {} segment {}'.format(test_data[0][0], '/'.join(test_data[0][1])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "line number: 0\n",
      "line number: 1000\n",
      "line number: 2000\n",
      "line number: 3000\n",
      "line number: 4000\n",
      "line number: 5000\n",
      "line number: 6000\n",
      "line number: 7000\n",
      "line number: 8000\n",
      "line number: 9000\n",
      "line number: 10000\n",
      "line number: 11000\n",
      "line number: 12000\n",
      "line number: 13000\n",
      "line number: 14000\n",
      "line number: 15000\n",
      "line number: 16000\n",
      "line number: 17000\n",
      "line number: 18000\n",
      "line number: 19000\n",
      "line number: 20000\n",
      "line number: 21000\n",
      "line number: 22000\n",
      "line number: 23000\n",
      "line number: 24000\n",
      "line number: 25000\n",
      "line number: 26000\n",
      "line number: 27000\n",
      "line number: 28000\n",
      "line number: 29000\n",
      "line number: 30000\n",
      "line number: 31000\n",
      "line number: 32000\n",
      "line number: 33000\n",
      "line number: 34000\n",
      "line number: 35000\n",
      "line number: 36000\n",
      "line number: 37000\n",
      "line number: 38000\n",
      "line number: 39000\n",
      "line number: 40000\n",
      "line number: 41000\n",
      "line number: 42000\n",
      "line number: 43000\n",
      "line number: 44000\n",
      "line number: 45000\n",
      "line number: 46000\n",
      "line number: 47000\n",
      "line number: 48000\n",
      "line number: 49000\n",
      "line number: 50000\n",
      "first training data: label 体育 segment 马晓旭/意外/受伤/让/国奥/警惕/ /无奈/大雨/格外/青睐/殷家/军/记者/傅亚雨/沈阳/报道/ /来到/沈阳/，/国奥队/依然/没有/摆脱/雨水/的/困扰/。/7/月/31/日/下午/6/点/，/国奥队/的/日常/训练/再度/受到/大雨/的/干扰/，/无奈/之下/队员/们/只/慢跑/了/25/分钟/就/草草收场/。/31/日/上午/10/点/，/国奥队/在/奥体中心/外场/训练/的/时候/，/天/就是/阴沉沉/的/，/气象预报/显示/当天/下午/沈阳/就/有/大雨/，/但/幸好/队伍/上午/的/训练/并/没有/受到/任何/干扰/。/下午/6/点/，/当/球队/抵达/训练场/时/，/大雨/已经/下/了/几个/小时/，/而且/丝毫/没有/停下来/的/意思/。/抱/着/试一试/的/态度/，/球队/开始/了/当天/下午/的/例行/训练/，/25/分钟/过去/了/，/天气/没有/任何/转好/的/迹象/，/为了/保护/球员/们/，/国奥队/决定/中止/当天/的/训练/，/全队/立即/返回/酒店/。/在/雨/中/训练/对/足球队/来说/并/不是/什么/稀罕/事/，/但/在/奥运会/即将/开始/之前/，/全队/变得/“/娇贵/”/了/。/在/沈阳/最后/一周/的/训练/，/国奥队/首先/要/保证/现有/的/球员/不再/出现意外/的/伤病/情况/以免/影响/正式/比赛/，/因此/这一/阶段/控制/训练/受伤/、/控制/感冒/等/疾病/的/出现/被/队伍/放在/了/相当/重要/的/位置/。/而/抵达/沈阳/之后/，/中/后卫/冯萧霆/就/一直/没有/训练/，/冯萧霆/是/7/月/27/日/在/长春/患上/了/感冒/，/因此/也/没有/参加/29/日/跟/塞尔维亚/的/热身赛/。/队伍/介绍/说/，/冯萧霆/并/没有/出现/发烧/症状/，/但/为了/安全/起/见/，/这/两天/还是/让/他/静养/休息/，/等/感冒/彻底/好/了/之后/再/恢复/训练/。/由于/有/了/冯萧霆/这个/例子/，/因此/国奥队/对雨中/训练/就/显得/特别/谨慎/，/主要/是/担心/球员/们/受凉/而/引发/感冒/，/造成/非战斗/减员/。/而/女足/队员/马晓旭/在/热身赛/中/受伤/导致/无缘/奥运/的/前科/，/也/让/在/沈阳/的/国奥队/现在/格外/警惕/，/“/训练/中/不断/嘱咐/队员/们/要/注意/动作/，/我们/可/不能/再出/这样/的/事情/了/。/”/一位/工作人员/表示/。/从/长春/到/沈阳/，/雨水/一路/伴随/着/国奥队/，/“/也/邪/了/，/我们/走/到/哪儿/雨/就/下/到/哪儿/，/在/长春/几次/训练/都/被/大雨/给/搅和/了/，/没想到/来/沈阳/又/碰到/这种/事情/。/”/一位/国奥/球员/也/对/雨水/的/“/青睐/”/有些/不解/。\n",
      "line number: 0\n",
      "line number: 1000\n",
      "line number: 2000\n",
      "line number: 3000\n",
      "line number: 4000\n",
      "line number: 5000\n",
      "line number: 6000\n",
      "line number: 7000\n",
      "line number: 8000\n",
      "line number: 9000\n",
      "first testing data: label 体育 segment 鲍勃/库西/奖归/谁/属/？/ /NCAA/最强/控卫/是/坎巴/还是/弗神/新浪/体育讯/如今/，/本赛季/的/NCAA/进入/到/了/末段/，/各项/奖项/的/评选/结果/也/即将/出炉/，/其中/评选/最佳/控卫/的/鲍勃/-/库西/奖/就/将/在/下周/最终/四强/战时/公布/，/鲍勃/-/库西/奖是/由奈/史密斯/篮球/名人堂/提供/，/旨在/奖励/年度/最佳/大学/控卫/。/最终/获奖/的/球员/也/即将/在/以下/几名/热门/人选/中/产生/。/〈/〈/〈/ /NCAA/疯狂/三月/专题/主页/上线/，/点击/链接/查看/精彩内容/吉梅尔/-/弗雷/戴特/，/杨百翰/大学/“/弗神/”/吉梅尔/-/弗雷/戴特/一直/都/备受/关注/，/他/不仅仅/是/一名/射手/，/他会用/“/终结/对手/脚踝/”/一样/的/变向/过/掉/面前/的/防守/者/，/并且/他/可以/用/任意/一支/手/完成/得分/，/如果/他/被/犯规/了/，/可以/提前/把/这/两份/划入/他/的/帐/下/了/，/因为/他/是/一名/命中率/高达/90%/的/罚球/手/。/弗雷/戴特/具有/所有/伟大/控卫/都/具备/的/一点/特质/，/他/是/一位/赢家/也/是/一位/领导者/。/“/他/整个/赛季/至始/至终/的/稳定/领导/着/球队/前进/，/这是/无可比拟/的/。/”/杨百翰/大学/主教练/戴夫/-/罗斯/称赞/道/，/“/他/的/得分/能力/毋庸置疑/，/但是/我/认为/他/带领/球队/获胜/的/能力/才/是/他/最/重要/的/控卫/职责/。/我们/在/主场/之外/的/比赛/(/客场/或/中/立场/)/共/取胜/19/场/，/他/都/表现/的/很棒/。/”/弗雷/戴特/能否/在/NBA/取得成功/？/当然/，/但是/有/很多/专业人士/比/我们/更/有/资格/去/做出/这样/的/判断/。/“/我/喜爱/他/。/”/凯尔特人/主教练/多克/-/里/弗斯/说道/，/“/他/很棒/，/我/看过/ESPN/的/片段/剪辑/，/从/剪辑/来看/，/他/是/个/超级/巨星/，/我/认为/他/很/成为/一名/优秀/的/NBA/球员/。/”/诺兰/-/史密斯/，/杜克大学/当/赛季/初/，/球队/宣布/大/一天/才/控卫凯瑞/-/厄尔/文因/脚趾/的/伤病/缺席/赛季/大部分/比赛/后/，/诺兰/-/史密斯/便/开始/接管/球权/，/他/在/进攻/端上/足/发条/，/在/ACC/联盟/(/杜克大学/所在/分区/)/的/得分/榜上/名列前茅/，/但/同时/他/在/分区/助攻/榜上/也/占据/头名/，/这/在/众强/林立/的/ACC/联盟/前无古人/。/“/我/不/认为/全美/有/其他/的/球员/能/在/凯瑞/-/厄尔/文/受伤/后/，/如此/好/的/接管/球队/，/并且/之前/毫无准备/。/”/杜克/主教练/迈克/-/沙舍/夫斯基/赞扬/道/，/“/他会/将/比赛/带入/自己/的/节奏/，/得分/，/组织/，/领导/球队/，/无所不能/。/而且/他/现在/是/攻防/俱佳/，/对/持球/人/的/防守/很/有/提高/。/总之/他/拥有/了/辉煌/的/赛季/。/”/坎巴/-/沃克/，/康涅狄格/大学/坎巴/-/沃克/带领/康涅狄格/在/赛季/初/的/毛伊岛/邀请赛/一路/力克/密歇根州/大/和/肯塔基/等队/夺冠/，/他场/均/30/分/4/助攻/得到/最佳/球员/。/在/大东/赛区/锦标赛/和/全国/锦标赛/中/，/他场/均/27.1/分/，/6.1/个/篮板/，/5.1/次/助攻/，/依旧/如此/给力/。/他/以/疯狂/的/表现/开始/这个/赛季/，/也/将/以/疯狂/的/表现/结束/这个/赛季/。/“/我们/在/全国/锦标赛/中/前进/着/，/并且/之前/曾经/5/天/连赢/5/场/，/赢得/了/大东/赛区/锦标赛/的/冠军/，/这些/都/归功于/坎巴/-/沃克/。/”/康涅狄格/大学/主教练/吉姆/-/卡洪/称赞/道/，/“/他/是/一名/纯正/的/控卫/而且/能为/我们/得分/，/他/有/过/单场/42/分/，/有过/单场/17/助攻/，/也/有/过/单场/15/篮板/。/这些/都/是/一名/6/英尺/175/镑/的/球员/所/完成/的/啊/！/我们/有/很多/好/球员/，/但/他/才/是/最好/的/领导者/，/为/球队/所/做/的/贡献/也/是/最大/。/”/乔丹/-/泰勒/，/威斯康辛/大学/全美/没有/一个/持球者/能/像/乔丹/-/泰勒/一样/很少/失误/，/他/4.26/的/助攻/失误/在/全美/遥遥领先/，/在/大十/赛区/的/比赛/中/，/他/平均/35.8/分钟/才/会/有/一次/失误/。/他/还是/名/很/出色/的/得分手/，/全场/砍/下/39/分/击败/印第安纳/大学/的/比赛/就是/最好/的/证明/，/其中/下半场/他/曾经/连/拿/18/分/。/“/那个/夜晚/他/证明/自己/值得/首轮/顺位/。/”/当时/的/见证者/印第安纳/大学/主教练/汤姆/-/克/雷恩/说道/。/“/对/一名/控卫/的/所有/要求/不过/是/领导/球队/、/使/球队/变/的/更好/、/带领/球队/成功/，/乔丹/-/泰勒/全/做到/了/。/”/威斯康辛/教练/博/-/莱恩/说道/。/诺里斯/-/科尔/，/克利夫兰/州/大/诺里斯/-/科尔/的/草根/传奇/正在/上演/，/默默无闻/的/他/被/克利夫兰/州/大/招募/后/便/开始/刻苦/地/训练/，/去年/夏天/他/曾/加练/上/千次/跳投/，/来/提高/这个/可能/的/弱点/。/他/在/本赛季/与/杨斯顿/州/大/的/比赛/中/得到/40/分/20/篮板/和/9/次/助攻/，/在/他/之前/，/过去/15/年/只有/一位/球员/曾经/在/NCAA/一级/联盟/做到/过/40/+/20/，/他/的/名字/是/布雷克/-/格里芬/。/“/他/可以/很/轻松/地防下/对方/王牌/。/”/克利夫兰/州/大/主教练/加里/-/沃特斯/如此/称赞/自己/的/弟子/，/“/同时/他/还/能/得分/，/并/为/球队/助攻/，/他/几乎/能/做到/一个/成功/的/团队/所有/需要/的/事/。/”/这/其中/四名/球员/都/带领/自己/的/球队/进入/到/了/甜蜜/16/强/，/虽然/有/3/个/球员/和/他们/各自/的/球队/被/挡/在/8/强/的/大门/之外/，/但是/他们/已经/表现/的/足够/出色/，/不远/的/将来/他们/很/可能/出现/在/一所/你/熟悉/的/NBA/球馆/里/。/(/clay/)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "在花费一些时间进行分词后，可以开始构建词典了，词典从训练集中构建，按照词频进行排序。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def build_vocab(train_data, thresh):\n",
    "    vocab = {'<UNK>': 0}\n",
    "    word_count = {} # word frequency\n",
    "    for idx, data in enumerate(train_data):\n",
    "        content = data[1]\n",
    "        for word in content:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    word_list = [(k, v) for k, v in word_count.items()]\n",
    "    print(f'word list length: {len(word_list)}')\n",
    "    word_list.sort(key = lambda x : x[1], reverse = True) # sorted by word frequency\n",
    "    word_list_filtered = [word for word in word_list if word[1] > thresh]\n",
    "    print(f'word list length after filtering: {len(word_list_filtered)}')\n",
    "\n",
    "    # construct vocab\n",
    "    for word in word_list_filtered:\n",
    "        vocab[word[0]]  = len(vocab)\n",
    "    print(f'vocab size : {len(vocab)}')\n",
    "    return vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "vocab = build_vocab(train_data, 1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word list length: 359255\n",
      "word list length after filtering: 208966\n",
      "vocab size : 208967\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "此外，根据category，我们知道标签本身也有个“词典”："
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def build_label_vocab(cate_file):\n",
    "    label_vocab = {}\n",
    "    with codecs.open(cate_file, 'r', 'utf-8') as f:\n",
    "        for lines in f:\n",
    "            line = lines.strip().split('\\t')\n",
    "            label_vocab[line[0]]  = int(line[1])\n",
    "    return label_vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "label_vocab = build_label_vocab('../resource/cnews/cnews.category.txt')\n",
    "print(f'label vocab: {label_vocab}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "label vocab: {'体育': 0, '娱乐': 1, '家居': 2, '房产': 3, '教育': 4, '时尚': 5, '时政': 6, '游戏': 7, '科技': 8, '财经': 9}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来构建id化的训练和测试集，因为我们只考虑bag of words，所以词的顺序被排除。构造成libsvm能吃的样子。注意，因为bag of word模型本 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def construct_trainable_matrix(corpus, vocab, label_vocab, out_file):\n",
    "    records = []\n",
    "    for idx, data in enumerate(corpus):\n",
    "        if idx % 1000 == 0:\n",
    "            print(f'process {idx} data')\n",
    "        label = str(label_vocab[data[0]]) # label id\n",
    "        token_dict = {}\n",
    "        for token in data[1]:\n",
    "            token_id = vocab.get(token, 0)\n",
    "            if token_id in token_dict:\n",
    "                token_dict[token_id] += 1\n",
    "            else:\n",
    "                token_dict[token_id] = 1\n",
    "        feature = [str(int(k) + 1) + ':' + str(v) for k, v in token_dict.items()]\n",
    "        feature_text = ' '.join(feature)\n",
    "        records.append(label + ' ' + feature_text)\n",
    "\n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write('\\n'.join(records))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "construct_trainable_matrix(train_data, vocab, label_vocab, '../resource/cnews/train.svm.txt')\n",
    "construct_trainable_matrix(test_data, vocab, label_vocab, '../resource/cnews/test.svm.txt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "process 0 data\n",
      "process 1000 data\n",
      "process 2000 data\n",
      "process 3000 data\n",
      "process 4000 data\n",
      "process 5000 data\n",
      "process 6000 data\n",
      "process 7000 data\n",
      "process 8000 data\n",
      "process 9000 data\n",
      "process 10000 data\n",
      "process 11000 data\n",
      "process 12000 data\n",
      "process 13000 data\n",
      "process 14000 data\n",
      "process 15000 data\n",
      "process 16000 data\n",
      "process 17000 data\n",
      "process 18000 data\n",
      "process 19000 data\n",
      "process 20000 data\n",
      "process 21000 data\n",
      "process 22000 data\n",
      "process 23000 data\n",
      "process 24000 data\n",
      "process 25000 data\n",
      "process 26000 data\n",
      "process 27000 data\n",
      "process 28000 data\n",
      "process 29000 data\n",
      "process 30000 data\n",
      "process 31000 data\n",
      "process 32000 data\n",
      "process 33000 data\n",
      "process 34000 data\n",
      "process 35000 data\n",
      "process 36000 data\n",
      "process 37000 data\n",
      "process 38000 data\n",
      "process 39000 data\n",
      "process 40000 data\n",
      "process 41000 data\n",
      "process 42000 data\n",
      "process 43000 data\n",
      "process 44000 data\n",
      "process 45000 data\n",
      "process 46000 data\n",
      "process 47000 data\n",
      "process 48000 data\n",
      "process 49000 data\n",
      "process 0 data\n",
      "process 1000 data\n",
      "process 2000 data\n",
      "process 3000 data\n",
      "process 4000 data\n",
      "process 5000 data\n",
      "process 6000 data\n",
      "process 7000 data\n",
      "process 8000 data\n",
      "process 9000 data\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练过程\n",
    "剩下的核心模型反而简单：使用libsvm来训练支持向量机，让你的svm吃进你处理好的训练和测试文件，然后掉用libsvm的现有方法训练就行，我们可以更换不同的参数设定。libsvm的文档可以查看[这里](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)，其中\"-s,-t,-c\"参数比较重要，他们决定你选择怎样的svm，你的核函数选择，你的惩罚系数。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from libsvm import svm\n",
    "from libsvm.svmutil import svm_read_problem, svm_train, svm_predict, svm_save_model,svm_load_model\n",
    "\n",
    "# train svm\n",
    "train_label, train_feature = svm_read_problem('../resource/cnews/train.svm.txt')\n",
    "print(train_label[0], train_feature[0])\n",
    "model = svm_train(train_label, train_feature, '-s 0 -c 5 -t 0 -g 0.5 -e 0.1')\n",
    "\n",
    "# predict\n",
    "test_label, test_feature = svm_read_problem('../resource/cnews/test.svm.txt')\n",
    "print(test_label[0], test_feature[0])\n",
    "p_labs, p_acc, p_vals = svm_predict(test_label, test_feature, model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0 {158685: 2.0, 1848: 1.0, 1562: 3.0, 56: 3.0, 12828: 2.0, 6198: 2.0, 9: 2.0, 2511: 2.0, 20324: 5.0, 4804: 2.0, 2645: 2.0, 1: 3.0, 4655: 1.0, 79: 1.0, 2457: 8.0, 273: 1.0, 1106: 1.0, 2: 38.0, 39883: 8.0, 670: 1.0, 54: 7.0, 4406: 1.0, 15446: 3.0, 3: 21.0, 3587: 1.0, 4: 14.0, 151: 2.0, 26: 2.0, 1162: 2.0, 47: 4.0, 1064: 4.0, 133: 2.0, 417: 3.0, 3648: 1.0, 1089: 13.0, 1328: 1.0, 592: 2.0, 4714: 2.0, 2332: 1.0, 2424: 3.0, 168: 4.0, 167: 1.0, 52111: 1.0, 7: 12.0, 676: 2.0, 661: 2.0, 27: 5.0, 65911: 1.0, 1595: 2.0, 103: 1.0, 5: 8.0, 46167: 1.0, 55879: 1.0, 154: 1.0, 871: 1.0, 84: 1.0, 104771: 1.0, 62933: 1.0, 247: 1.0, 1358: 3.0, 15: 2.0, 42: 3.0, 10139: 1.0, 2032: 3.0, 60: 3.0, 439: 2.0, 277: 1.0, 280: 2.0, 2765: 2.0, 15550: 1.0, 68: 1.0, 66: 1.0, 108: 2.0, 761: 1.0, 644: 1.0, 239: 1.0, 4656: 1.0, 15360: 1.0, 2976: 1.0, 3210: 1.0, 125: 2.0, 28511: 1.0, 1153: 1.0, 115: 2.0, 6990: 1.0, 451: 1.0, 2629: 1.0, 14677: 1.0, 2267: 1.0, 262: 2.0, 832: 1.0, 374: 4.0, 421: 1.0, 16494: 1.0, 2605: 2.0, 2148: 1.0, 3533: 1.0, 1370: 1.0, 7663: 2.0, 24: 4.0, 29: 2.0, 33886: 1.0, 288: 1.0, 143: 1.0, 185: 1.0, 31590: 1.0, 828: 1.0, 2775: 1.0, 774: 1.0, 326: 1.0, 1071: 1.0, 10: 4.0, 88340: 1.0, 11: 4.0, 243: 1.0, 1583: 1.0, 683: 1.0, 50: 2.0, 899: 1.0, 2256: 1.0, 1100: 1.0, 43825: 1.0, 2842: 1.0, 153: 1.0, 6586: 1.0, 205: 1.0, 511: 1.0, 127: 1.0, 255: 3.0, 458: 1.0, 760: 1.0, 766: 2.0, 6: 1.0, 10239: 4.0, 48: 2.0, 3610: 1.0, 146: 2.0, 55: 2.0, 1636: 1.0, 692: 1.0, 211: 1.0, 788: 1.0, 39: 3.0, 172: 2.0, 3061: 1.0, 116695: 4.0, 223: 1.0, 8: 2.0, 944: 1.0, 9872: 3.0, 11801: 1.0, 14: 4.0, 384: 1.0, 1169: 1.0, 269: 1.0, 32154: 1.0, 46168: 2.0, 337: 1.0, 43: 1.0, 15157: 1.0, 7561: 1.0, 660: 1.0, 630: 1.0, 1950: 1.0, 38: 1.0, 3308: 1.0, 126: 1.0, 21: 1.0, 47550: 1.0, 2357: 1.0, 2358: 1.0, 92: 1.0, 182: 1.0, 1223: 1.0, 209: 1.0, 63: 1.0, 3194: 1.0, 911: 1.0, 287: 1.0, 2022: 1.0, 196: 1.0, 938: 1.0, 133260: 1.0, 1273: 1.0, 494: 1.0, 69219: 1.0, 40733: 1.0, 32694: 1.0, 552: 1.0, 8961: 1.0, 3843: 1.0, 44953: 1.0, 101: 1.0, 411: 1.0, 37293: 1.0, 923: 1.0, 887: 1.0, 28: 2.0, 148: 1.0, 240: 1.0, 33887: 1.0, 122: 1.0, 550: 2.0, 409: 2.0, 1020: 1.0, 69: 1.0, 57: 1.0, 37: 3.0, 2598: 1.0, 3044: 1.0, 22455: 1.0, 481: 1.0, 6713: 2.0, 3632: 1.0, 22: 1.0, 107: 1.0, 60395: 1.0, 2893: 1.0, 93: 1.0, 105: 1.0, 4198: 1.0, 157: 1.0, 446: 1.0, 13336: 1.0}\n",
      "0.0 {16638: 3.0, 41762: 3.0, 1: 9.0, 609: 1.0, 3930: 1.0, 53: 2.0, 9: 1.0, 8908: 4.0, 3922: 1.0, 3402: 7.0, 8: 14.0, 52420: 4.0, 126: 2.0, 73856: 2.0, 159: 1.0, 711: 1.0, 802: 1.0, 2: 75.0, 1011: 2.0, 3: 58.0, 241: 2.0, 37: 2.0, 7: 7.0, 19274: 1.0, 2557: 1.0, 3021: 1.0, 3149: 2.0, 472: 1.0, 14: 7.0, 774: 2.0, 5237: 1.0, 174: 3.0, 813: 3.0, 36: 24.0, 2005: 1.0, 27: 1.0, 25: 3.0, 5: 19.0, 6314: 1.0, 368: 2.0, 23213: 1.0, 23760: 1.0, 388: 1.0, 46633: 1.0, 2506: 3.0, 2960: 1.0, 18404: 1.0, 213: 1.0, 4037: 1.0, 1611: 1.0, 1221: 1.0, 139: 8.0, 4: 32.0, 2909: 1.0, 374: 9.0, 653: 1.0, 11282: 1.0, 2071: 1.0, 4613: 1.0, 24: 6.0, 489: 1.0, 18609: 3.0, 23: 1.0, 1416: 3.0, 7338: 1.0, 4157: 1.0, 24699: 1.0, 4181: 1.0, 2259: 1.0, 4983: 1.0, 3326: 1.0, 12577: 1.0, 96918: 2.0, 24110: 4.0, 50933: 4.0, 30270: 2.0, 10: 14.0, 11: 14.0, 223: 1.0, 22: 6.0, 3037: 1.0, 309: 1.0, 21: 36.0, 2168: 1.0, 838: 6.0, 10283: 1.0, 58348: 1.0, 4495: 1.0, 697: 1.0, 4978: 1.0, 278: 2.0, 58234: 1.0, 192: 4.0, 2322: 1.0, 1847: 1.0, 573: 2.0, 1085: 1.0, 559: 3.0, 45: 3.0, 175: 1.0, 7342: 1.0, 2142: 1.0, 2214: 2.0, 347: 2.0, 795: 6.0, 100: 1.0, 55: 3.0, 1205: 1.0, 987: 1.0, 114: 1.0, 38: 3.0, 14375: 1.0, 28666: 1.0, 21171: 1.0, 108: 2.0, 113: 1.0, 1576: 1.0, 1161: 1.0, 3914: 1.0, 2216: 1.0, 430: 1.0, 299: 3.0, 3186: 1.0, 1211: 1.0, 521: 1.0, 6453: 1.0, 409: 3.0, 8964: 1.0, 13572: 2.0, 408: 1.0, 679: 7.0, 96426: 1.0, 105716: 1.0, 583: 1.0, 1265: 3.0, 125: 2.0, 280: 12.0, 5658: 2.0, 294: 1.0, 31163: 1.0, 2712: 6.0, 27083: 1.0, 2529: 1.0, 7351: 3.0, 1415: 3.0, 265: 2.0, 12068: 1.0, 112: 3.0, 20: 5.0, 123: 3.0, 2612: 4.0, 5206: 1.0, 235: 4.0, 90: 1.0, 211: 1.0, 5804: 1.0, 28: 5.0, 955: 1.0, 1112: 2.0, 127: 6.0, 19: 3.0, 1102: 1.0, 155: 1.0, 4780: 1.0, 18: 3.0, 1109: 1.0, 5069: 1.0, 704: 1.0, 826: 2.0, 232: 4.0, 9525: 2.0, 1657: 1.0, 1078: 3.0, 13213: 1.0, 455: 1.0, 15: 9.0, 119: 2.0, 7000: 1.0, 187: 1.0, 64: 1.0, 1649: 1.0, 129: 1.0, 984: 1.0, 122: 1.0, 1432: 1.0, 2483: 1.0, 1737: 1.0, 34581: 1.0, 144: 2.0, 9388: 1.0, 3397: 3.0, 2667: 1.0, 5854: 1.0, 7293: 1.0, 8308: 2.0, 57: 1.0, 500: 1.0, 72: 3.0, 1585: 1.0, 3576: 1.0, 51: 5.0, 132: 1.0, 1033: 1.0, 34694: 2.0, 24908: 2.0, 277: 1.0, 3530: 2.0, 743: 1.0, 94: 6.0, 1057: 1.0, 17094: 2.0, 15472: 1.0, 2842: 1.0, 3278: 1.0, 941: 1.0, 59: 3.0, 726: 2.0, 115: 3.0, 12836: 2.0, 15456: 1.0, 618: 1.0, 135241: 1.0, 6958: 1.0, 36739: 1.0, 66255: 2.0, 701: 3.0, 2170: 1.0, 11284: 2.0, 19628: 2.0, 14682: 1.0, 42: 2.0, 150: 2.0, 637: 7.0, 2496: 1.0, 18568: 1.0, 194655: 1.0, 18011: 1.0, 42902: 1.0, 35: 1.0, 3789: 3.0, 230: 1.0, 76: 4.0, 32297: 1.0, 1210: 1.0, 1562: 1.0, 606: 3.0, 92: 2.0, 326: 3.0, 118742: 1.0, 16219: 1.0, 5070: 1.0, 69235: 1.0, 40743: 1.0, 14279: 1.0, 4112: 1.0, 11279: 1.0, 58: 4.0, 2552: 1.0, 652: 1.0, 24845: 1.0, 239: 2.0, 101: 1.0, 6813: 1.0, 31699: 1.0, 29: 2.0, 7272: 1.0, 44: 1.0, 422: 2.0, 5072: 1.0, 356: 1.0, 4053: 1.0, 15290: 3.0, 23465: 3.0, 131413: 1.0, 16425: 1.0, 2598: 1.0, 17186: 1.0, 29012: 1.0, 12: 4.0, 11433: 1.0, 96072: 1.0, 5138: 1.0, 11238: 2.0, 397: 2.0, 297: 1.0, 83: 6.0, 106: 1.0, 233: 2.0, 5137: 3.0, 8937: 4.0, 351: 2.0, 104795: 1.0, 32799: 1.0, 363: 3.0, 19146: 1.0, 339: 2.0, 2106: 1.0, 8455: 1.0, 67: 2.0, 63: 3.0, 463: 1.0, 754: 3.0, 97: 2.0, 871: 1.0, 73331: 1.0, 1873: 1.0, 1526: 1.0, 124: 2.0, 13345: 1.0, 14879: 1.0, 104860: 1.0, 11488: 1.0, 8500: 1.0, 7890: 3.0, 4114: 1.0, 5118: 1.0, 598: 1.0, 342: 2.0, 133: 1.0, 3153: 1.0, 19871: 1.0, 23419: 1.0, 188: 2.0, 968: 1.0, 95: 1.0, 625: 2.0, 31: 2.0, 111: 1.0, 1230: 1.0, 308: 1.0, 2582: 3.0, 5618: 3.0, 24689: 2.0, 54: 1.0, 33: 2.0, 252: 1.0, 1725: 1.0, 2084: 3.0, 144337: 1.0, 12296: 1.0, 117452: 1.0, 651: 1.0, 117392: 1.0, 661: 1.0, 41: 1.0, 286: 1.0, 587: 1.0, 1437: 2.0, 8543: 1.0, 1422: 1.0, 2204: 1.0, 3939: 1.0, 2163: 1.0, 11711: 2.0, 84: 1.0, 805: 2.0, 3195: 1.0, 857: 1.0, 468: 1.0, 505: 1.0, 822: 1.0, 7266: 1.0, 1128: 1.0, 5958: 1.0, 12121: 1.0, 349: 1.0, 46346: 1.0, 9250: 1.0, 4188: 1.0, 34074: 1.0, 208: 1.0, 179: 1.0, 6: 2.0, 386: 1.0, 1776: 1.0, 896: 1.0, 370: 2.0, 1291: 1.0, 1380: 3.0, 2141: 1.0, 11056: 1.0, 17214: 1.0, 69659: 2.0, 14128: 2.0, 8625: 3.0, 5528: 4.0, 10407: 1.0, 2202: 1.0, 380: 1.0, 2194: 1.0, 23242: 1.0, 7364: 1.0, 11957: 1.0, 88: 1.0, 1089: 1.0, 313: 1.0, 2250: 1.0, 318: 1.0, 18102: 1.0, 32: 1.0, 137300: 1.0, 2574: 1.0, 93: 1.0, 86: 2.0, 12475: 1.0, 34: 1.0, 885: 2.0, 249: 2.0, 177: 1.0, 451: 1.0, 30: 1.0, 259: 1.0, 3136: 1.0, 527: 1.0, 1508: 1.0, 5757: 1.0, 2355: 1.0, 1061: 1.0, 1238: 1.0, 10252: 1.0, 8034: 1.0, 69755: 1.0, 9283: 1.0, 52: 1.0, 60: 1.0, 610: 1.0, 926: 1.0, 137: 1.0, 828: 1.0, 15168: 1.0, 4599: 1.0, 499: 1.0, 1062: 2.0, 201: 1.0, 71: 1.0, 61: 3.0, 2555: 1.0, 4562: 1.0, 156: 1.0, 4831: 1.0, 66: 1.0, 1381: 1.0, 9451: 1.0, 1774: 1.0, 146: 1.0, 3432: 1.0, 49: 1.0, 1823: 1.0, 6332: 1.0, 58608: 1.0}\n",
      "Accuracy = 94.43% (9443/10000) (classification)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print(f'accuracy: {p_acc}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy: (94.43, 0.8064, 0.9065002155823192)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "经过一段时间的训练，我们就可以观察到实验结果了，你可以更换不同的svm类型、惩罚系数和核函数，把结果调优。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('tensorflow': conda)"
  },
  "interpreter": {
   "hash": "32732e49ddba1da8b293d0354da9484fcc377d4ba69e79a82935691a68d0ecf6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}