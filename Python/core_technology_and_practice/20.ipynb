{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "CPU times: user 6.05 ms, sys: 3.01 ms, total: 9.07 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "# 一个简单的爬虫栗子\n",
    "import time\n",
    " \n",
    "def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    time.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        crawl_page(url)\n",
    "\n",
    "%time main(['url_1', 'url_2', 'url_3', 'url_4'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "# 并发化, 使用协程\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    for url in urls:\n",
    "        await crawl_page(url)\n",
    "\n",
    "await main(['url_1', 'url_2', 'url_3', 'url_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    \"\"\"\n",
    "    我们要等所有任务都结束才行，用for task in tasks: await task 即可。\n",
    "    \"\"\"\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    for task in tasks:\n",
    "        await task\n",
    "\n",
    "await main(['url_1', 'url_2', 'url_3', 'url_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "# 另外一种做法\n",
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    \"\"\"\n",
    "    要注意的是，*tasks 解包列表，将列表变成了函数的参数；与之对应的是， ** dict 将字典变成了函数的参数。\n",
    "    \"\"\"\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "await main(['url_1', 'url_2', 'url_3', 'url_4'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before await\n",
      "worker_1 start\n",
      "worker_1 done\n",
      "await worker_1\n",
      "worker_2 start\n",
      "worker_2 done\n",
      "await worker_2\n"
     ]
    }
   ],
   "source": [
    "# 深入代码底层\n",
    "import asyncio\n",
    "\n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(1)\n",
    "    print('worker_1 done')\n",
    "\n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    "\n",
    "async def main():\n",
    "    print('before await')\n",
    "    await worker_1()\n",
    "    print('await worker_1')\n",
    "    await worker_2()\n",
    "    print('await worker_2')\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before await\n",
      "worker_1 start\n",
      "worker_2 start\n",
      "worker_1 done\n",
      "awaited worker1\n",
      "worker_2 done\n",
      "awaited worker2\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(1)\n",
    "    print('worker_1 done')\n",
    "\n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    "\n",
    "async def main():\n",
    "    task1 = asyncio.create_task(worker_1())\n",
    "    task2 = asyncio.create_task(worker_2())\n",
    "\n",
    "    print('before await')\n",
    "    await task1\n",
    "    print('awaited worker1')\n",
    "    await task2\n",
    "    print('awaited worker2')\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, ZeroDivisionError('division by zero'), CancelledError('')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "如果我们想给某些协程任务限定运行时间，一旦超时就取消，又该怎么做呢？再进一步，如果某些协程运行时出现错误，又该怎么处理呢？\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def worker_1():\n",
    "    await asyncio.sleep(1)\n",
    "    return 1\n",
    "\n",
    "async def worker_2():\n",
    "    await asyncio.sleep(2)\n",
    "    return 2/0\n",
    "\n",
    "async def worker_3():\n",
    "    await asyncio.sleep(3)\n",
    "    return 3\n",
    "\n",
    "async def main():\n",
    "    task_1 = asyncio.create_task(worker_1())\n",
    "    task_2 = asyncio.create_task(worker_2())\n",
    "    task_3 = asyncio.create_task(worker_3())\n",
    "\n",
    "    await asyncio.sleep(2)\n",
    "    task_3.cancel()\n",
    "\n",
    "    \"\"\"\n",
    "    return_exceptions = True, \n",
    "    如果不设置这个参数，错误就会完整地 throw 到我们这个执行层，\n",
    "    从而需要 try except 来捕捉，这也就意味着其他还没被执行的任务会被全部取消掉。\n",
    "    为了避免这个局面，我们将 return_exceptions 设置为 True 即可。\n",
    "    \"\"\"\n",
    "    res = await asyncio.gather(task_1, task_2, task_3, return_exceptions=True)\n",
    "    print(res)\n",
    "\n",
    "await main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producer_1 put a val: 10\n",
      "producer_2 put a val: 6\n",
      "consumer_1 get a val: 10\n",
      "consumer_2 get a val: 6\n",
      "producer_1 put a val: 7\n",
      "producer_2 put a val: 2\n",
      "consumer_1 get a val: 7\n",
      "consumer_2 get a val: 2\n",
      "producer_1 put a val: 9\n",
      "producer_2 put a val: 2\n",
      "consumer_1 get a val: 9\n",
      "consumer_2 get a val: 2\n",
      "producer_1 put a val: 1\n",
      "producer_2 put a val: 6\n",
      "consumer_1 get a val: 1\n",
      "consumer_2 get a val: 6\n",
      "producer_1 put a val: 6\n",
      "producer_2 put a val: 1\n",
      "consumer_1 get a val: 6\n",
      "consumer_2 get a val: 1\n"
     ]
    }
   ],
   "source": [
    "# 协程实现生产者消费者模型\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "\n",
    "async def consumer(queue, id):\n",
    "    while True:\n",
    "        val = await queue.get()\n",
    "        print('{} get a val: {}'.format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "async def producer(queue, id):\n",
    "    for i in range(5):\n",
    "        val = random.randint(1, 10)\n",
    "        await queue.put(val)\n",
    "        print('{} put a val: {}'.format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "async def main():\n",
    "    queue = asyncio.Queue()\n",
    "\n",
    "    consumer_1 = asyncio.create_task(consumer(queue, 'consumer_1'))\n",
    "    consumer_2 = asyncio.create_task(consumer(queue, 'consumer_2'))\n",
    "\n",
    "    producer_1 = asyncio.create_task(producer(queue, 'producer_1'))\n",
    "    producer_2 = asyncio.create_task(producer(queue, 'producer_2'))\n",
    "\n",
    "    await asyncio.sleep(10)\n",
    "    consumer_1.cancel()\n",
    "    consumer_2.cancel()\n",
    "\n",
    "    await asyncio.gather(consumer_1, consumer_2, producer_1, producer_2, return_exceptions=True)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最初的梦想 01月07日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2766852789.jpg\n",
      "魔法满屋 01月07日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2807936075.jpg\n",
      "一江春水 01月07日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2779904200.jpg\n",
      "独家头条 01月07日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2812894770.jpg\n",
      "冰上时刻 01月07日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2791542382.jpg\n",
      "屋内有人 01月07日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2755165106.jpg\n",
      "萌鸡小队：萌闯新世界 01月08日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2781907425.jpg\n",
      "追梦少年 01月08日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2674282554.jpg\n",
      "农民院士 01月09日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2799267344.jpg\n",
      "黑客帝国：矩阵重启 01月14日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2761285836.jpg\n",
      "东北虎 01月14日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2812275146.jpg\n",
      "汪汪队立大功大电影 01月14日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2808318200.jpg\n",
      "纹身 01月14日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2810333367.jpg\n",
      "花滑女王2：爸爸我爱你 01月14日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2815263713.jpg\n",
      "爱情的代驾 01月14日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2766667694.jpg\n",
      "向着明亮那方 01月15日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2796840848.jpg\n",
      "我们的冬奥 01月15日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2770923285.jpg\n",
      "四海 02月01日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2688377102.jpg\n",
      "奇迹 02月01日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2813379601.jpg\n",
      "狙击手 02月01日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2738601191.jpg\n",
      "超能一家人 02月01日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2793293939.jpg\n",
      "我心飞扬 02月01日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2672325162.jpg\n",
      "这个杀手不太冷静 02月01日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2814578241.jpg\n",
      "熊出没·重返地球 02月01日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2740221448.jpg\n",
      "喜羊羊与灰太狼之筐出未来 02月01日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2797468943.jpg\n",
      "我是霸王龙 02月01日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2799075912.jpg\n",
      "唐刀记 02月01日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2622162804.jpg\n",
      "小虎墩大英雄 02月01日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2792787666.jpg\n",
      "戏如人生 02月01日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2635687482.jpg\n",
      "CPU times: user 1.33 s, sys: 67.4 ms, total: 1.4 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "# 豆瓣今日推荐电影爬虫\n",
    "# https://movie.douban.com/cinema/later/beijing/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    head = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36',\n",
    "        'Referer':'https://time.geekbang.org/column/article/101855',\n",
    "        'Connection':'keep-alive'\n",
    "    }\n",
    "    init_page = requests.get(url, headers=head).content\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    all_movies = init_soup.find('div', id = 'showing-soon')\n",
    "    for each_movie in all_movies.find_all('div', class_='item'):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "\n",
    "        movie_name = all_a_tag[1].text\n",
    "        url_to_fetch = all_a_tag[1]['href']\n",
    "        movie_date = all_li_tag[0].text\n",
    "\n",
    "        response_item = requests.get(url_to_fetch, headers=head).content\n",
    "        soup_item = BeautifulSoup(response_item, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    "        \n",
    "        # print(movie_name, movie_date, response_item)\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "%time main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ClientResponse(https://movie.douban.com/cinema/later/beijing/) [200 OK]>\n",
      "<CIMultiDictProxy('Date': 'Fri, 31 Dec 2021 17:59:09 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Keep-Alive': 'timeout=30', 'Set-Cookie': 'bid=f-x3-ngLg84; Expires=Sat, 31-Dec-22 17:59:09 GMT; Domain=.douban.com; Path=/', 'X-DOUBAN-NEWBID': 'f-x3-ngLg84', 'Server': 'dae', 'Strict-Transport-Security': 'max-age=15552000', 'X-Content-Type-Options': 'nosniff', 'Content-Encoding': 'gzip')>\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h4/7cr1cmpn7v5b3x20_9wz8m740000gn/T/ipykernel_78150/1573517486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} {} {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/h4/7cr1cmpn7v5b3x20_9wz8m740000gn/T/ipykernel_78150/1573517486.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mall_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'showing-soon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0meach_movie\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_movies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mall_a_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_movie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mall_li_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_movie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def fetch_content(url):\n",
    "    header  = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36',\n",
    "        'Referer':'https://time.geekbang.org/column/article/101855',\n",
    "        'Connection':'keep-alive'\n",
    "    }\n",
    "    async with aiohttp.ClientSession(\n",
    "        headers=header, connector=aiohttp.TCPConnector(ssl=False)\n",
    "    ) as session:\n",
    "        async with session.get(url) as response:\n",
    "            print(response)\n",
    "            return await response.text()\n",
    "\n",
    "async def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    header  = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36',\n",
    "        'Referer':'https://time.geekbang.org/column/article/101855',\n",
    "        'Connection':'keep-alive'\n",
    "    }\n",
    "    init_page = await fetch_content(url)\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    movie_names, urls_to_fetch, movie_dates = [], [], []\n",
    "\n",
    "    all_movies = init_soup.find('div', id='showing-soon')\n",
    "\n",
    "    for each_movie in all_movies.find_all('div', class_='item'):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "\n",
    "        movie_names.append(all_a_tag[1].text)\n",
    "        urls_to_fetch.append(all_a_tag[1]['href'])\n",
    "        movie_dates.append(all_li_tag[0].text)\n",
    "\n",
    "    tasks = [fetch_content(url) for url in urls_to_fetch]\n",
    "    pages = await asyncio.gather(*tasks)\n",
    "\n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = BeautifulSoup(page, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h4/7cr1cmpn7v5b3x20_9wz8m740000gn/T/ipykernel_78150/2046919330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} {} {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/h4/7cr1cmpn7v5b3x20_9wz8m740000gn/T/ipykernel_78150/2046919330.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mall_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"showing-soon\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0meach_movie\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_movies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"item\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mall_a_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_movie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mall_li_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_movie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def fetch_content(url, header):\n",
    "    async with aiohttp.ClientSession(\n",
    "        headers=header, connector=aiohttp.TCPConnector(ssl=False)\n",
    "    ) as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    header  = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36',\n",
    "        'Referer':'https://time.geekbang.org/column/article/101855',\n",
    "        'Connection':'keep-alive'\n",
    "    }\n",
    "    init_page = await fetch_content(url, header)\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    movie_names, urls_to_fetch, movie_dates = [], [], []\n",
    "\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "\n",
    "        movie_names.append(all_a_tag[1].text)\n",
    "        urls_to_fetch.append(all_a_tag[1]['href'])\n",
    "        movie_dates.append(all_li_tag[0].text)\n",
    "\n",
    "    tasks = [fetch_content(url) for url in urls_to_fetch]\n",
    "    pages = await asyncio.gather(*tasks)\n",
    "\n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = BeautifulSoup(page, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3745030874.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/h4/7cr1cmpn7v5b3x20_9wz8m740000gn/T/ipykernel_78150/3745030874.py\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    divs = soup.find_all('div', class='item mod')\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "now = lambda: time.perf_counter()\n",
    "\n",
    "async def fetchHtmlText(url):\n",
    "    async with aiohttp.ClientSession(\n",
    "        headers = {'users-agent': 'Mozilla/5.0'},\n",
    "        connector=aiohttp.TCPConnector(ssl=False)\n",
    "    ) as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def main():\n",
    "    url = 'https://movie.douban.com/cinema/later/beijing/'\n",
    "    html = await fetchHtmlText(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    divs = soup.find_all('div', class_='item mod')\n",
    "    urls = list(map(lambda x: x.a.img['src', divs]))\n",
    "    names = list(map(lambda x: x.h3.a.string, divs))\n",
    "    dats = list(map(lambda x: x.ul.li.string, divs))\n",
    "\n",
    "    lis = zip(names, dats, urls)\n",
    "    for i in lis:\n",
    "        print(\"{0:{3}^25}\\t{1:{3}^10}\\t{2:{3}^}\".format(i[0], i[1], i[2], chr(12288)))\n",
    "\n",
    "start = now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "354f827198093f365559b09f495968767f485f877b8fc4b69f4afdbf0a5233ed"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
