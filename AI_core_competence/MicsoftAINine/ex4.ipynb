{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 4 循环神经网络，文本表征，词向量初步，文本自动分类"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRNN Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 文本分类，RNN模型\n",
    "class TextRNN(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        # 进行词嵌入\n",
    "        self.embedding = nn.Embedding(5000, 64)  \n",
    "        # self.rnn = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        self.rnn = nn.GRU(input_size=64, hidden_size=128, num_layers=2, bidirectional=True)\n",
    "        self.f1 = nn.Sequential(nn.Linear(256,128),\n",
    "                                nn.Dropout(0.8),\n",
    "                                nn.ReLU())\n",
    "        self.f2 = nn.Sequential(nn.Linear(128,10),\n",
    "                                nn.Softmax())\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x,_ = self.rnn(x)\n",
    "        x = F.dropout(x,p=0.8)\n",
    "        x = self.f1(x[:,-1,:])\n",
    "        return self.f2(x)\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# cnews loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 读取词汇表\n",
    "def read_vocab(vocab_dir):\n",
    "    with open(vocab_dir, 'r', encoding='utf-8', errors='ignore') as fp:\n",
    "        words = [_.strip() for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 读取分类目录，固定\n",
    "def read_category():\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "    categories = [x for x in categories]\n",
    "    cat_to_id = dict(zip(categories, range(len(categories)))) \n",
    "    return categories, cat_to_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 将文件转换为id表示\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    contents, labels = [], []\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(content))\n",
    "                    labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])#将每句话id化\n",
    "        label_id.append(cat_to_id[labels[i]])#每句话对应的类别的id\n",
    "    #\n",
    "    # # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "    #\n",
    "    return x_pad, y_pad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 获取文本的类别及其对应id的字典\n",
    "categories, cat_to_id = read_category()\n",
    "print(categories)\n",
    "# 获取训练文本中所有出现过的字及其所对应的id\n",
    "words, word_to_id = read_vocab('../resource/cnews/cnews.vocab.txt')\n",
    "#print(words)\n",
    "#print(word_to_id)\n",
    "#print(word_to_id)\n",
    "#获取字数\n",
    "vocab_size = len(words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 数据加载及分批\n",
    "# 获取训练数据每个字的id和对应标签的one-hot形式\n",
    "x_train, y_train = process_file('../resource/cnews/cnews.train.txt', word_to_id, cat_to_id, 600)\n",
    "print('x_train=', x_train)\n",
    "x_val, y_val = process_file('../resource/cnews/cnews.val.txt', word_to_id, cat_to_id, 600)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train= [[1609  659   56 ...    9  311    3]\n",
      " [   2  101   16 ... 1168    3   24]\n",
      " [ 465  855  521 ...  116  136   85]\n",
      " ...\n",
      " [  49   18   79 ...  836 1928 1072]\n",
      " [ 166  110  714 ...  836 1928 1072]\n",
      " [   1   80  551 ...   78  192    3]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 测试"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import tensorflow.contrib.keras as kr\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "vocab_file = '../resource/cnews/cnews.vocab.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class RnnModel:\n",
    "    def __init__(self):\n",
    "        self.categories, self.cat_to_id = read_category()\n",
    "        self.words, self.word_to_id = read_vocab(vocab_file)\n",
    "        self.model = TextRNN()\n",
    "        self.model.load_state_dict(torch.load('model_params.pkl'))\n",
    " \n",
    "    def predict(self, message):\n",
    "        content = message\n",
    "        data = [self.word_to_id[x] for x in content if x in self.word_to_id]\n",
    "        data = kr.preprocessing.sequence.pad_sequences([data], 600)\n",
    "        \n",
    "        data = torch.LongTensor(data)\n",
    "        y_pred_cls = self.model(data)\n",
    "        class_index = torch.argmax(y_pred_cls[0]).item()\n",
    "        return self.categories[class_index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    model = RnnModel()\n",
    "    test_demo = ['《时光重返四十二难》恶搞唐增取经一款时下最热门的动画人物：猪猪侠，加上创新的故事背景，震撼的操作快感，成就了这部恶搞新作，现正恶搞上市，玩家们抢先赶快体验快感吧。游戏简介：被时光隧道传送到208年的猪猪侠，必须经历六七四十二难的考验，才能借助柯伊诺尔大钻石的力量，开启时光隧道，重返2008年。在迷糊老师、菲菲公主的帮助下，猪猪侠接受了挑战，开始了这段充满了关心和情谊的旅程。    更多精彩震撼感觉，立即下载该款游戏尽情体验吧。玩家交流才是王道，讯易游戏玩家交流中心 QQ群：6306852-----------------生活要有激情，游戏要玩多彩(多彩游戏)。Colourfulgame (多彩游戏)，让你看看快乐游戏的颜色！精品推荐：1：《钟馗传》大战无头关羽，悲壮的剧情伴随各朝英灵反攻地府！2：《中华群英》将和赵云，项羽，岳飞等猛将作战，穿越各朝代抗击日寇。良品推荐：1：《赌王争霸之斗地主》易飞会在四角恋中会选择谁？是否最终成赌神呢？2：勇者后裔和魔王紧缠一起，前代恩怨《圣火伏魔录》将为您揭示一切。  3：颠覆传统概念，恶搞+非主流？！誓必弄死搞残为止《爆笑飞行棋》。4：《中国象棋残局大师》快棋和人机模式让畅快对弈！一切“多彩游戏”资讯，点击Colourfulgame官网http://www.colourfulgame.com一切“多彩游戏”感言，交流Colourfulgame论坛http://121.33.203.124/forum/【客服邮箱】：xunyiwangluo@126.com\">xunyiwangluo@126.com\">xunyiwangluo@126.com【客服热线】：020-87588437']\n",
    "                 \n",
    "    for i in test_demo:\n",
    "        print(i,\":\",model.predict(i))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 将数据集变成小样本"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "filename = '../resource/cnews/cnews.train.txt'\n",
    "num_cat = {}\n",
    "num_max = 100\n",
    "\n",
    "# 读取文件\n",
    "contents, labels = [], []\n",
    "with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        label, content = line.strip().split('\\t')\n",
    "        #print(label)\n",
    "        if content:\n",
    "            if label not in num_cat:\n",
    "                num_cat[label] = 1\n",
    "                contents.append(content)\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                if num_cat[label] < 100:\n",
    "                    num_cat[label] = num_cat[label] + 1\n",
    "                    contents.append(content)\n",
    "                    labels.append(label)\n",
    "\n",
    "# 写文件\n",
    "with open('../resource/cnews/cnews.train.small.txt', 'w', encoding='utf-8', errors='ignore') as f:\n",
    "    for content, label in zip(contents, labels):\n",
    "        f.write(label + '\\t' + content+'\\n')\n",
    "    f.close()\n",
    "print(len(contents))\n",
    "print(contents[0])\n",
    "print(contents[1])\n",
    "print(num_cat)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n",
      "马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有摆脱雨水的困扰。7月31日下午6点，国奥队的日常训练再度受到大雨的干扰，无奈之下队员们只慢跑了25分钟就草草收场。31日上午10点，国奥队在奥体中心外场训练的时候，天就是阴沉沉的，气象预报显示当天下午沈阳就有大雨，但幸好队伍上午的训练并没有受到任何干扰。下午6点，当球队抵达训练场时，大雨已经下了几个小时，而且丝毫没有停下来的意思。抱着试一试的态度，球队开始了当天下午的例行训练，25分钟过去了，天气没有任何转好的迹象，为了保护球员们，国奥队决定中止当天的训练，全队立即返回酒店。在雨中训练对足球队来说并不是什么稀罕事，但在奥运会即将开始之前，全队变得“娇贵”了。在沈阳最后一周的训练，国奥队首先要保证现有的球员不再出现意外的伤病情况以免影响正式比赛，因此这一阶段控制训练受伤、控制感冒等疾病的出现被队伍放在了相当重要的位置。而抵达沈阳之后，中后卫冯萧霆就一直没有训练，冯萧霆是7月27日在长春患上了感冒，因此也没有参加29日跟塞尔维亚的热身赛。队伍介绍说，冯萧霆并没有出现发烧症状，但为了安全起见，这两天还是让他静养休息，等感冒彻底好了之后再恢复训练。由于有了冯萧霆这个例子，因此国奥队对雨中训练就显得特别谨慎，主要是担心球员们受凉而引发感冒，造成非战斗减员。而女足队员马晓旭在热身赛中受伤导致无缘奥运的前科，也让在沈阳的国奥队现在格外警惕，“训练中不断嘱咐队员们要注意动作，我们可不能再出这样的事情了。”一位工作人员表示。从长春到沈阳，雨水一路伴随着国奥队，“也邪了，我们走到哪儿雨就下到哪儿，在长春几次训练都被大雨给搅和了，没想到来沈阳又碰到这种事情。”一位国奥球员也对雨水的“青睐”有些不解。\n",
      "商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典多曼来了，瑞典来了，商瑞华首战求3分的信心也来了。距离首战72小时当口，中国女足彻底从“恐瑞症”当中获得解脱，因为商瑞华已经找到了瑞典人的软肋。找到软肋，保密4月20日奥运会分组抽签结果出来后，中国姑娘就把瑞典锁定为关乎奥运成败的头号劲敌，因为除了浦玮等个别老将之外，现役女足将士竟然没有人尝过击败瑞典的滋味。在中瑞两队共计15次交锋的历史上，中国队6胜3平6负与瑞典队平分秋色，但从2001年起至今近8年时间，中国在同瑞典连续5次交锋中均未尝胜绩，战绩为2平3负。尽管八年不胜瑞典曾一度成为谢亚龙聘请多曼斯基的理由之一，但这份战绩表也成为压在姑娘们身上的一座大山，在奥运备战过程中越发凸显沉重。或许正源于此，商瑞华才在首战前3天召开了一堂完全针对“恐瑞症”的战术分析课。3日中午在喜来登大酒店中国队租用的会议室里，商瑞华给大家播放了瑞典队的比赛录像剪辑。这是7月6日瑞典在主场同美国队进行的一场奥运热身赛，当时美国队头牌射手瓦姆巴赫还没有受伤，比赛当中双方均尽遣主力，占据主场优势的瑞典队曾一度占据上风，但终究还是在定位球防守上吃了亏，美国队在下半场的一次角球配合中，通过远射打进唯一进球，尽管慢动作显示是打在瑞典队后卫腿上弹射入网，但从过程到结果，均显示了相同内容——“瑞典队的防守并非无懈可击”。商瑞华让科研教练曹晓东等人对这场比赛进行精心剪辑，尤其是瑞典队失球以及美国队形成有威胁射门的片段，更是被放大进行动作分解，每一个中国姑娘都可以一目了然地看清瑞典队哪些地方有机可乘。甚至之前被商瑞华称为“恐怖杀手”的瑞典8号谢琳，也在这次战术分解过程中被发现了不足之处。姑娘们心知肚明并开心享受对手软肋被找到的欢悦，但却必须对记者保密，某主力球员说：“这可不能告诉外界，反正我们心里有数了，知道对付这个速度奇快的谢琳该怎么办！”老帅的“瑞典情结”就像中国队8年不胜瑞典一样，瑞典队也连续遭遇对美国队的溃败：去年阿尔加夫杯瑞典0比1不敌美国，世界杯小组赛上瑞典0比2被美国完胜，今年阿尔加夫杯美国人再次击败瑞典，算上7月6日一役，瑞典遇到美国连平局都没有，竟然是4连败！3日中午的这堂战术分析课后，“用美国人的方式击败瑞典”已经成为中国女足将士的共同心声。姑娘们当然有理由这样去憧憬，因为在7月30日奥运会前最后一场热身赛中，中国女足便曾0比0与强大的美国队握手言和。在3日中午的战术分析课上，这场中美热身也被梳理出片段，与姑娘们一道完成总结。点评过程中，商瑞华对大家在同美国队比赛中表现出来的逼抢意识，给予很高评价。主帅的认可和称赞，更是在大家潜意识里强化了“逼抢的价值”，就连排在18人名单外的候补球员都说：“既然我们能跟最擅长逼抢的美国队玩逼抢，当然有信心跟瑞典队也这么打”，姑娘们都对打好首战的信心越来越强。“我们当然需要低调，但内心深处已经再也没有对瑞典的恐惧，只要把我们训练中的内容打出来，就完全有可能击败瑞典”，这堂战术讨论课后，不止一名球员向记者表达着对首战获胜的渴望。商瑞华心中的“瑞典情结”其实最重，不仅仅因为他是主帅，17年前商瑞华首次担任中国女足主帅时所遭遇的滑铁卢，正是源自1991年世界杯中国0比1被瑞典淘汰所致。抽签出来后面对与瑞典同组，64岁的老帅那份复仇的雄心也潜滋暗长，在5月上旬和7月中旬，商瑞华曾连续两次前往欧洲在现场刺探瑞典军情，真正对瑞典队的特点达到了如指掌的程度。昨天中午的战术讨论课后，商瑞华告诉记者：“瑞典队和所有其他队伍一样有优点也有不足，我们如果能够扬长避短拿对方的不足做文章，就有可能击败对手。从奥运会备战阶段看来，中国队战术上打得比较快的特点基本已经成型，边路也不错，在前场抢截后快速发动进攻的能力也逐步增强。我很清楚瑞典队世界排名第3而我们排第14，但承认差距不等于接受失败，我当然想赢。”\n",
      "{'体育': 100, '娱乐': 100, '家居': 100, '房产': 100, '教育': 100, '时尚': 100, '时政': 100, '游戏': 100, '科技': 100, '财经': 100}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 使用GRU/LSTM进行文本分类"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.utils.data as Data #将数据分批次需要用到它"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "\n",
    "# 设置数据集目录\n",
    "#train_file = '../resource/cnews/cnews.train.small.txt'\n",
    "train_file = '../resource/cnews/cnews.train.txt'\n",
    "test_file = '../resource/cnews/cnews.test.txt'\n",
    "val_file = '../resource/cnews/cnews.val.txt'\n",
    "vocab_file = '../resource/cnews/cnews.vocab.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def train():\n",
    "    # 使用LSTM或者CNN\n",
    "    model = TextRNN().cuda()\n",
    "    # 选择损失函数\n",
    "    Loss = nn.MultiLabelSoftMarginLoss()\n",
    "    #Loss = nn.CrossEntropyLoss()\n",
    "    #Loss = nn.MSELoss() # 多分类一般不用MSE\n",
    "    optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "    #optimizer = optim.Adam(model.parameters(),lr=5e-4)\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    # 加载之前训练过的网络参数\n",
    "    if os.path.exists('model_params.pkl'):\n",
    "        model.load_state_dict(torch.load('model_params.pkl'))\n",
    "        \n",
    "    for epoch in range(1000):\n",
    "        print('epoch=', epoch)\n",
    "        # 将训练集分批batch\n",
    "        for step, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x = x_batch.cuda()\n",
    "            y = y_batch.cuda()\n",
    "            #print('x=', x)\n",
    "            out = model(x)\n",
    "            #print('out=', out)\n",
    "            #print('y', y)\n",
    "            loss = Loss(out,y)\n",
    "            print('loss=', loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).cpu().numpy())\n",
    "            print(accuracy)\n",
    "        #对模型进行验证\n",
    "        if (epoch+1)%5 == 0:\n",
    "            for step, (x_batch, y_batch) in enumerate(val_loader):\n",
    "                x = x_batch.cuda()\n",
    "                y = y_batch.cuda()\n",
    "                # 前向传播\n",
    "                out = model(x)\n",
    "                accuracy = np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).cpu().numpy())\n",
    "                if accuracy > best_val_acc:\n",
    "                    torch.save(model.state_dict(), 'model_params.pkl')\n",
    "                    best_val_acc = accuracy\n",
    "                    print('model_params.pkl saved')\n",
    "                print(accuracy)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "\n",
    "# 获取文本的类别及其对应id的字典\n",
    "categories, cat_to_id = read_category()\n",
    "#print(categories)\n",
    "#print(cat_to_id)\n",
    "# 获取训练文本中所有出现过的字及其所对应的id\n",
    "words, word_to_id = read_vocab(vocab_file)\n",
    "#print(words)\n",
    "#print(word_to_id)\n",
    "#获取字数\n",
    "vocab_size = len(words)\n",
    "#print(vocab_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# 数据加载及分批\n",
    "# 获取训练数据每个字的id和对应标签的one-hot形式\n",
    "x_train, y_train = process_file(train_file, word_to_id, cat_to_id, 600)\n",
    "print('x_train=', x_train)\n",
    "x_val, y_val = process_file(val_file, word_to_id, cat_to_id, 600)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train= [[1609  659   56 ...    9  311    3]\n",
      " [   2  101   16 ... 1168    3   24]\n",
      " [ 465  855  521 ...  116  136   85]\n",
      " ...\n",
      " [  49   18   79 ...  836 1928 1072]\n",
      " [ 166  110  714 ...  836 1928 1072]\n",
      " [   1   80  551 ...   78  192    3]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 设置使用GPU\n",
    "cuda = torch.device('cuda')\n",
    "x_train, y_train = torch.LongTensor(x_train), torch.Tensor(y_train)\n",
    "x_val, y_val = torch.LongTensor(x_val), torch.Tensor(y_val)\n",
    "print(x_train)\n",
    "\n",
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,# torch TensorDataset format\n",
    "    batch_size=1000,      # 最新批数据\n",
    "    shuffle=True,         # 是否随机打乱数据\n",
    "    num_workers=2,        # 用于加载数据的子进程\n",
    ")\n",
    "\n",
    "val_dataset = Data.TensorDataset(x_val, y_val)\n",
    "val_loader = Data.DataLoader(\n",
    "    dataset=val_dataset,  # torch TensorDataset format\n",
    "    batch_size=1000,      # 最新批数据\n",
    "    shuffle=True,         # 是否随机打乱数据\n",
    "    num_workers=2,        # 用于加载数据的子进程\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "## Mnist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 超参数定义\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEP = 28\n",
    "INPUT_SIZE = 28\n",
    "LR = 0.001"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# MNIST数据集下载\n",
    "train_data = datasets.MNIST(root='../resource', train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download = True\n",
    "                           )\n",
    "test_data = datasets.MNIST(root='../resource', train=False, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download = True\n",
    "                           )\n",
    "\n",
    "test_x = test_data.test_data.type(torch.FloatTensor)[:2000] / 255.\n",
    "test_y = test_data.test_labels.numpy()[:2000]\n",
    "\n",
    "print(train_data.train_data.size())\n",
    "print(train_data.train_labels.size())\n",
    "plt.imshow(train_data.train_data[0].numpy(), cmap='gray')\n",
    "plt.show()\n",
    "#print(train_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../resource/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "9913344it [00:01, 5846159.03it/s]                             \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../resource/MNIST/raw/train-images-idx3-ubyte.gz to ../resource/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../resource/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "29696it [00:00, 622530.58it/s]           \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../resource/MNIST/raw/train-labels-idx1-ubyte.gz to ../resource/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../resource/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1649664it [00:00, 4205751.07it/s]                            \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../resource/MNIST/raw/t10k-images-idx3-ubyte.gz to ../resource/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../resource/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "5120it [00:00, 25234825.48it/s]         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../resource/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../resource/MNIST/raw\n",
      "\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 使用Dataloader进行分批\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 定义网络\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=INPUT_SIZE, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.out = nn.Linear(64, 10) #10个分类\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播\n",
    "        r_out, _ = self.rnn(x)\n",
    "        # 选择 r_out的最后一个时间步\n",
    "        out = self.out(r_out[:,-1,:])\n",
    "        return out\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 设置使用GPU\n",
    "cuda = torch.device('cuda')\n",
    "rnn = RNN()\n",
    "rnn = rnn.cuda()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 训练&验证\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):\n",
    "        b_x = b_x.view(-1, 28, 28)\n",
    "        # 前向传播\n",
    "        output = rnn(b_x.cuda())\n",
    "        # 损失函数\n",
    "        loss = loss_func(output, b_y.cuda())\n",
    "        # 后向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            test_output = rnn(test_x.cuda())\n",
    "            pred_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "            # 计算准确率\n",
    "            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\n",
    "            print ('Epoch: {}, Step: {}, loss: {}, accuracy: {}'.format(epoch, step, loss, accuracy))\n",
    "\n",
    "# 从测试集选出10个，进行验证\n",
    "test_x = test_x.cuda()\n",
    "test_output = rnn(test_x[:10].view(-1, 28, 28))\n",
    "pred_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "print('预测数字', pred_y)\n",
    "print('实际数字', test_y[:10])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "35f69483aa8b7b2bb12f1e6f2ffb8fe1120cf837ca5a80070d69961a5682f06d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}